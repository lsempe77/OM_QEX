# OM_QEX - Outcome Mapping Quality of Evidence Exchange

A curated dataset of 95 papers on poverty graduation programs with full-text extractions and LLM-based data extraction tools.

## ğŸ“ Structure

```
OM_QEX/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # 3 CSV metadata files
â”‚   â”œâ”€â”€ human_extraction/     # Manual extractions (ground truth)
â”‚   â””â”€â”€ grobid_outputs/       # 95 papers Ã— 2 formats (TEI XML + TXT)
â”œâ”€â”€ om_qex_extraction/        # ğŸ†• LLM-based extraction app
â”‚   â”œâ”€â”€ src/                  # Extraction engine and parsers
â”‚   â”œâ”€â”€ prompts/              # LLM extraction prompts
â”‚   â”œâ”€â”€ config/               # Configuration (API keys)
â”‚   â””â”€â”€ outputs/              # Extracted data (JSON + CSV)
â”œâ”€â”€ scripts/                  # Data processing scripts
â””â”€â”€ outputs/                  # Analysis results
```

## ğŸ“Š Dataset

**95 included studies** on poverty graduation and ultra-poor programs

### Raw Data (`data/raw/`)
- **Master file (n=95)** - Primary dataset with study metadata
- **fulltext_metadata** - Links paper IDs to GROBID outputs

### Human Extraction (`data/human_extraction/`)
- **Manual data extraction** - Ground truth for comparison with LLM extraction
- **Prompt engineering input** - Reference data for developing extraction prompts
- **Quality benchmark** - Validation standard for automated extraction

### Full-Text Outputs (`data/grobid_outputs/`)
- **tei/** - 95 TEI XML files (structured with sections, references, metadata)
- **text/** - 95 plain text files (cleaned full-text extraction)

## ğŸ› ï¸ Tools & Scripts

### LLM Extraction Application (`om_qex_extraction/`) â­

**Automated quantitative data extraction from research papers using LLMs.**

#### ğŸš€ Quick Start
```powershell
cd om_qex_extraction

# Test on 2 papers with human ground truth
python run_extraction.py --keys PHRKN65M ABM3E3ZP

# Compare with human extraction
python compare_extractions.py

# Review results
notepad outputs\comparison\comparison_report.txt
```

**Expected baseline**: ~35% agreement (7/20 fields) on test papers.

#### ğŸ“š Documentation
- **[TESTING_WORKFLOW.md](om_qex_extraction/TESTING_WORKFLOW.md)** - Complete testing guide (START HERE)
- **[TEST_RESULTS.md](om_qex_extraction/TEST_RESULTS.md)** - Current baseline & findings
- **[QUICK_REFERENCE.md](om_qex_extraction/QUICK_REFERENCE.md)** - Commands cheat sheet
- **[COMPARISON_GUIDE.md](om_qex_extraction/COMPARISON_GUIDE.md)** - Understanding comparison results
- **[EXTRACTION_READY.md](om_qex_extraction/EXTRACTION_READY.md)** - Full system documentation

#### âœ… System Status (Nov 10, 2025)
- **Extraction**: Working end-to-end with OpenRouter (Claude 3.5 Haiku)
- **Comparison**: LLM vs human validation system complete
- **Test papers**: 2 papers with human ground truth (PHRKN65M, ABM3E3ZP)
- **Baseline agreement**: 35% (7/20 fields)
- **Perfect match fields**: study_id, year, country, intervention_year, 3 graduation components
- **Known issues**: Multiple outcomes per paper, component disagreements, format differences
- **Next steps**: Quick wins (+7%), medium effort (+7%), major refactor (+25%) â†’ potential 75% agreement

#### ğŸ”§ Features
- âœ… TEI XML parser for GROBID outputs
- âœ… 15 core extraction fields + 7 graduation components
- âœ… Batch processing with retry logic
- âœ… JSON + CSV output formats
- âœ… LLM vs human comparison tool
- âœ… Content-based field matching (not character-based)
- âœ… Handles 0/1 â†’ Yes/No normalization
- âœ… Multiple comparison modes (numeric, categorical, text, component)

#### ğŸ“Š Extracted Fields
- **Bibliographic**: study_id, author_name, year_of_publication
- **Intervention**: program_name, country, year_intervention_started
- **Outcome**: outcome_name, outcome_description, evaluation_design
- **Statistics**: sample_size_treatment, sample_size_control, effect_size, p_value
- **Graduation Components** (7): consumption_support, healthcare, assets, skills_training, savings, coaching, social_empowerment

See `om_qex_extraction/README.md` for full documentation and usage examples.

---

### Data Processing Scripts (`scripts/`)

**Utility scripts for data management and analysis:**
**Utility scripts for data management and analysis:**

- `add_key_column.py` - Links paper IDs to GROBID Keys via fulltext_metadata
- `copy_files_by_key.py` - Extracts GROBID outputs for specific paper Keys
- `analyze_extraction_fields.py` - Analyzes human extraction CSV structure
- `get_human_study_ids.py` - Lists studies in human extraction dataset
- `map_ids_to_keys.py` - Maps study IDs to GROBID Keys for testing

---

### Diagnostic Scripts (`archive/`)

**Historical diagnostic scripts used during data cleaning:**

- `find_duplicate_keys.py` - Found duplicate study (121475488) sharing same Key
- `remove_duplicate.py` - Cleaned master file from 96 â†’ 95 studies
- `test_stem.py` - Diagnosed Path.stem behavior with .tei.xml files
- `check_121498842_human.py` - Verified study 121498842 not in master file
- Other diagnostic tools from data validation phase

These scripts are archived for reference but not needed for normal use.

---

## ğŸš€ Quick Start

### View the Dataset

### View the Dataset

```bash
# Clone the repository
git clone https://github.com/lsempe77/OM_QEX.git
cd OM_QEX

# View master dataset
# data/raw/Master file of included studies (n=95) 10 Nov(data).csv

# Access full-text files
# data/grobid_outputs/tei/  (95 TEI XML files - structured)
# data/grobid_outputs/text/ (95 TXT files - plain text)
```

### Test LLM Extraction

```powershell
cd om_qex_extraction

# Extract 2 test papers (with human ground truth)
python run_extraction.py --keys PHRKN65M ABM3E3ZP

# Compare against human extraction
python compare_extractions.py

# View results
notepad outputs\comparison\comparison_report.txt
```

See **[TESTING_WORKFLOW.md](om_qex_extraction/TESTING_WORKFLOW.md)** for complete testing guide.

### Run Full Extraction

```powershell
cd om_qex_extraction

# Setup API key (first time only)
cp config/config.yaml.template config/config.yaml
# Edit config.yaml and add your OpenRouter API key

# Install dependencies
pip install -r requirements.txt

# Extract all 95 papers (~10-15 min, ~$0.50-1.00)
python run_extraction.py --all
```

---

---

## ğŸ”— Linking IDs to Files

Papers have two identifiers:
- **Study ID** (e.g., 121058352) - Used in master file and human extraction
- **Key** (e.g., CV27ZK8Q) - Used for GROBID filenames

**To find GROBID files for a paper:**

1. Look up Study ID in `data/raw/fulltext_metadata.csv`
2. Find corresponding Key in the same row
3. Access files: `data/grobid_outputs/tei/[Key].tei.xml` and `data/grobid_outputs/text/[Key].txt`

**Example:**
```
Study ID: 121058352 (Bandiera 2009)
â†’ fulltext_metadata.csv: Key = CV27ZK8Q
â†’ Files: CV27ZK8Q.tei.xml, CV27ZK8Q.txt
```

**Shortcut**: Use `data/raw/Master file (n=95) 10 Nov(data).csv` which already has Key column merged.

---

## ğŸ“Š Test Papers (Human Ground Truth)

For LLM validation, 3 studies have manual human extraction:

| Study ID | Key | Author | Year | Program | Country | Status |
|----------|-----|--------|------|---------|---------|--------|
| 121294984 | PHRKN65M | Burchi & Strupat | 2018 | TEEP | Malawi | âœ… In master (9 outcomes) |
| 121058364 | ABM3E3ZP | Maldonado et al. | 2019 | SOF | Paraguay | âœ… In master |
| 121498842 | - | Mahecha et al. | - | SOF | Paraguay | âŒ Not in master |

**Only 2/3 papers can be tested** (121498842 was excluded from final dataset).

See `om_qex_extraction/TESTING_WORKFLOW.md` for testing details.

---

## ğŸ“ Notes

## ğŸ“ Notes

- **Dataset**: 95 poverty graduation program studies (cleaned from 96 - duplicate removed)
- **Full-text processing**: GROBID PDF extraction â†’ TEI XML + plain text
- **ID linking**: All Study IDs mapped to Keys via fulltext_metadata.csv
- **LLM extraction**: Claude 3.5 Haiku via OpenRouter API
- **Human validation**: 2 test papers with ground truth (35% baseline agreement)
- **Status**: System working end-to-end, ready for iterative improvement

## ğŸ“‚ Repository Contents

```
OM_QEX/
â”œâ”€â”€ README.md                          # This file - project overview
â”œâ”€â”€ DOCUMENTATION_UPDATE.md            # Documentation changelog (Nov 10, 2025)
â”œâ”€â”€ EXTRACTION_PLAN.md                 # Original extraction planning document
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”‚
â”œâ”€â”€ data/                              # Dataset files
â”‚   â”œâ”€â”€ README.md                      # Data documentation with test papers
â”‚   â”œâ”€â”€ raw/                           # Metadata CSVs
â”‚   â”‚   â”œâ”€â”€ Master file (n=95).csv     # Primary dataset âœ…
â”‚   â”‚   â””â”€â”€ fulltext_metadata.csv      # ID â†’ Key mapping
â”‚   â”œâ”€â”€ human_extraction/              # Ground truth (3 studies, 2 in master)
â”‚   â””â”€â”€ grobid_outputs/                # Full-text extractions (95 Ã— 2)
â”‚       â”œâ”€â”€ tei/                       # TEI XML (structured)
â”‚       â””â”€â”€ text/                      # Plain text
â”‚
â”œâ”€â”€ om_qex_extraction/                 # LLM extraction application â­
â”‚   â”œâ”€â”€ README.md                      # App documentation
â”‚   â”œâ”€â”€ TESTING_WORKFLOW.md            # Complete testing guide
â”‚   â”œâ”€â”€ TEST_RESULTS.md                # Current baseline & findings
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md             # Commands cheat sheet
â”‚   â”œâ”€â”€ COMPARISON_GUIDE.md            # Understanding results
â”‚   â”œâ”€â”€ EXTRACTION_READY.md            # System documentation
â”‚   â”œâ”€â”€ run_extraction.py              # Main extraction CLI
â”‚   â”œâ”€â”€ compare_extractions.py         # LLM vs human comparison
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â”œâ”€â”€ src/                           # Source code
â”‚   â”‚   â”œâ”€â”€ models.py                  # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ tei_parser.py              # TEI XML parser
â”‚   â”‚   â”œâ”€â”€ extraction_engine.py       # LLM extraction logic
â”‚   â”‚   â””â”€â”€ comparer.py                # Comparison system
â”‚   â”œâ”€â”€ prompts/                       # LLM prompts
â”‚   â”œâ”€â”€ config/                        # Configuration files
â”‚   â””â”€â”€ outputs/                       # Extraction results (gitignored)
â”‚
â”œâ”€â”€ scripts/                           # Utility scripts
â”‚   â”œâ”€â”€ add_key_column.py              # ID â†’ Key mapping
â”‚   â”œâ”€â”€ copy_files_by_key.py           # File extraction
â”‚   â”œâ”€â”€ get_human_study_ids.py         # List test papers
â”‚   â””â”€â”€ map_ids_to_keys.py             # ID â†’ Key lookup
â”‚
â””â”€â”€ archive/                           # Diagnostic scripts (historical)
    â”œâ”€â”€ find_duplicate_keys.py         # Found duplicate study
    â”œâ”€â”€ remove_duplicate.py            # Cleaned master file
    â”œâ”€â”€ test_stem.py                   # Path.stem diagnostics
    â””â”€â”€ ...                            # Other data cleaning tools
```

## ğŸ” Key Files

- **Start here**: `om_qex_extraction/TESTING_WORKFLOW.md`
- **Master dataset**: `data/raw/Master file of included studies (n=95) 10 Nov(data).csv`
- **Test results**: `om_qex_extraction/TEST_RESULTS.md`
- **Run extraction**: `om_qex_extraction/run_extraction.py`
- **Compare results**: `om_qex_extraction/compare_extractions.py`

---

## ğŸ¤ Contributing

This is a research dataset with LLM extraction tools. For questions or improvements:
- Review existing documentation in `om_qex_extraction/`
- Check `TEST_RESULTS.md` for known issues and improvement roadmap
- Follow `TESTING_WORKFLOW.md` for testing changes

## ğŸ“„ License

[Add license information]

---

**Last updated**: November 10, 2025  
**Dataset version**: 95 studies (duplicate removed)  
**Extraction system**: Working baseline established (35% agreement)  
**Repository**: https://github.com/lsempe77/OM_QEX
